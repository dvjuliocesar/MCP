# ğŸŒ Aula 04 â€” MCP: Coleta, Armazenamento e Processamento

## ğŸ§² ExercÃ­cio 1: Coleta de Dados (Guia + CÃ³digo)

Este projeto demonstra **duas formas de coleta**:
- **Scraping web** de preÃ§os (site de treino: `Books to Scrape` â€” e-commerce fictÃ­cio, seguro para estudos).
- **Cliente de API pÃºblica** (Open-Meteo) para sÃ©ries horÃ¡rias meteorolÃ³gicas.

> Foco: conexÃµes seguras, *timeouts*, *retries*, *backoff*, registro de logs, tratamento de erros e documentaÃ§Ã£o dos **campos extraÃ­dos**.

### â–¶ï¸ 1) Como executar
```bash
python -m venv .venv
# Ative a venv (Windows: .venv\Scripts\activate | Linux/Mac: source .venv/bin/activate)
pip install -r requirements.txt
cp .env .env
python -m src.main
```
SaÃ­das em `./data`.

### 2) Campos extraÃ­dos
#### ğŸ—‚ï¸ Produtos (scraper)
`source, product_name, price_gbp, availability, rating_1to5, url, scraped_at`

#### ğŸ—‚ï¸ Meteorologia (API)
`city, time, temperature_2m, relative_humidity_2m, precipitation, wind_speed_10m`

## ğŸ—„ï¸ ExercÃ­cio 2: Armazenamento de Dados (PostgreSQL + MongoDB)

Este exercÃ­cio complementa a Parte 1 (coleta) e mostra como persistir os CSVs em dois tipos de bancos:

- **PostgreSQL** â†’ modelo estrela mÃ­nimo (dimensÃµes + fatos)

- **MongoDB** â†’ documentos aninhados (produtos com histÃ³rico de preÃ§os e clima horÃ¡rio)

### 1) ğŸ“‹ PrÃ©-requisitos:

- CSVs gerados no ExercÃ­cio 1, na pasta `../MCP/data/`

- PostgreSQL e MongoDB rodando (local ou Docker)

- Python 3.10+ com pacotes de requirements instalados
```bash
pip install -r requirements.txt
```
### 2) âš™ï¸ ConfiguraÃ§Ã£o (`.env`)
```ini
# Onde estÃ£o os CSVs do ExercÃ­cio 1
DATA_DIR=../data

# PostgreSQL
PG_HOST=localhost
PG_PORT=5432
PG_DB=mcp
PG_USER=postgres
PG_PASSWORD=postgres123

# MongoDB
MONGO_URI=mongodb://localhost:27017
MONGO_DB=mcp
```
### 3) ğŸ˜ PostgreSQL 

- Criar banco e tabelas
```bash
psql -h localhost -U postgres -d postgres -c "CREATE DATABASE mcp;"
psql -h localhost -U postgres -d aula04 -f sql/postgres/schema.sql
psql -h localhost -U postgres -d aula04 -f sql/postgres/indexes.sql
```
- â–¶ï¸ Como executar (ETL)
```bash
python -m src.load_to_postgres
```
### 4) ğŸƒ MongoDB

- â–¶ï¸ Como executar (ETL)
```bash
python -m src.load_to_mongo
```
## âš™ï¸ ExercÃ­cio 3: Pipeline de Processamento (File + â€œStreamâ€)
Este exercÃ­cio complementa as Partes 1 e 2 com um **pipeline de transformaÃ§Ã£o** para gerar **dados curados, agregaÃ§Ãµes, alertas e relatÃ³rios** a partir dos CSVs de produtos e clima.

- **File Processor (batch)** â†’ lÃª CSV/JSON de uma pasta, aplica limpeza + normalizaÃ§Ã£o + agregaÃ§Ãµes + detecÃ§Ã£o de anomalias, e salva as saÃ­das.

- **Stream Processor (near real-time)** â†’ observa uma pasta; quando entra/Ã© alterado um CSV, reexecuta o pipeline automaticamente.

1) ğŸ“‹ PrÃ©-requisitos

- CSVs do ExercÃ­cio 1 em ../aula04_mcp_coleta/data/ (ou outra pasta sua).

- Python 3.10+ e dependÃªncias instaladas.

2) ğŸ§± O que o pipeline faz (resumo)

**Produtos/PreÃ§os**

- Converte datas para UTC, tipa colunas numÃ©ricas.

- Regras de plausibilidade: `price_gbp >= 0` , `rating_1to5 âˆˆ [1,5]`.

- Normaliza `url` e deduplica por (`url,scraped_at`).

- AgregaÃ§Ã£o diÃ¡ria por URL: `min/avg/max/last/n_obs`.

- Anomalias: z-score do delta de preÃ§o entre leituras consecutivas.

**Clima (hourly)**

- Datas UTC + tipagem numÃ©rica.

- Plausibilidade: `temperature [-80,80]`, `humidity [0,100]`, `precip >= 0`, `wind >= 0`.

- Dedup por (`city,time`).

- AgregaÃ§Ã£o diÃ¡ria por cidade: `avg_temp/avg_rh/total_precip/avg_wind/n_obs`.

- Anomalias: z-score de temperatura e IQR para precipitaÃ§Ã£o.

**SaÃ­das geradas**
```lua
output/
â”œâ”€ curated/
â”‚  â”œâ”€ products_curated.parquet
â”‚  â””â”€ weather_hourly_curated.parquet
â”œâ”€ agg/
â”‚  â”œâ”€ daily_price_stats.csv
â”‚  â””â”€ weather_daily_stats.csv
â”œâ”€ alerts/
â”‚  â”œâ”€ price_anomalies.csv
â”‚  â””â”€ weather_anomalies.csv
â””â”€ reports/
   â”œâ”€ chart_price_counts.png
   â”œâ”€ chart_temp_daily.png
   â””â”€ report.md
```

3) â–¶ï¸ ExecuÃ§Ã£o em lote (File Processor)

```bash
python -m src.processor_file --data-dir ../aula04_mcp_coleta/data --out-dir ./output
```

4) ğŸ” ExecuÃ§Ã£o â€œem tempo quase realâ€ (Stream Processor)

```bash
python -m src.stream_processor --watch-dir ../aula04_mcp_coleta/data --out-dir ./output --poll-seconds 5
```